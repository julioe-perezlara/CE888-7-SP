{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1QWts4Kplj24mYyW8rlTL8Iizy1bwiggY?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SemEval-2018 Task 3: Irony Detection\n",
    "\n",
    "This is the dataset called TweetEval [[1]](#section_id) and it is available [here](https://github.com/cardiffnlp/tweeteval) <br>\n",
    "\n",
    "As described by [[1]](#section_id), the tweets were retrieved with the\n",
    "Twitter API from October 2015 to February 2017 and ‚Äùgeolocalized‚Äù in United States. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Allow us to work with CSV files\n",
    "import emoji # Allow us to print Emojis\n",
    "import numpy as np # Allow us to work with arrays\n",
    "import re  # Allow us to work with regular expressions\n",
    "import nltk.data  # Allow to use the tokenizer punkt/english.pickle\n",
    "import nltk # import the nltk package\n",
    "from nltk.stem.snowball import SnowballStemmer # Import the SnowballStemmer algorithm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # Allow to disable Python warnings\n",
    "\n",
    "# PENDIENTE\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression # Import logistic regression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, make_scorer, f1_score, classification_report\n",
    "# Import scikit-learn.metrics module for accuracy score, make_scorer, confusion matrix and classification_report\n",
    "from sklearn import metrics # Import scikit-learn metrics module for Recall calculation\n",
    "from sklearn.model_selection import cross_val_score # Import cross validation score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "# Import train_test_split function, stratified K-Folds cross-validator and GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Preparing the Irony Detection Mapping.\n",
    "For this task, the **Irony Detection Mapping** proposed in the TweetEval [[1]](#section_id) will be used. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mIrony Detection Mapping Subset:\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Output</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>non_irony</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>irony</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Output  Label\n",
       "0  non_irony      0\n",
       "1      irony      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print the Irony Detection Mapping dataframe\n",
    "\n",
    "# Read a TXT file from internet (Github) that does not have a header\n",
    "df_irony_mapping = pd.read_csv('https://raw.githubusercontent.com/cardiffnlp/tweeteval/'\n",
    "                               'main/datasets/irony/mapping.txt', header=None, sep ='\\t', \n",
    "                                names=['Label','Output'], index_col=False)\n",
    "    \n",
    "print('\\n\\033[1mIrony Detection Mapping Subset:\\033[0m')\n",
    "df_irony_mapping = df_irony_mapping[['Output', 'Label']]\n",
    "display(df_irony_mapping) # Output: SemEval-2018 Irony Detection Mapping dataframe (truncated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above dataframe can be explaned with the following examples: <br>\n",
    "<br>\n",
    "‚Ä¢ As shown, 0 is equal to **Non Irony**, while 1 is equal to **Irony** <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Importing Irony Detection Train and Test subsets\n",
    "For this task, the **Irony Detection train and test subset** proposed in the TweetEval [[1]](#section_id) will be used. <br>\n",
    "Each dataset contains 2,862 tweets and 784 tweets correspondigly. They represent the feature variable (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mIrony Detection Train Subset:\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>seeing ppl walking w/ crutches makes me really...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>look for the girl with the broken smile, ask h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Now I remember why I buy books online @user #s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@user @user So is he banded from wearing the c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just found out there are Etch A Sketch apps.  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2857</th>\n",
       "      <td>I don't have to respect your beliefs.||I only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2858</th>\n",
       "      <td>Women getting hit on by married managers at @u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2859</th>\n",
       "      <td>@user no but i followed you and i saw you post...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2860</th>\n",
       "      <td>@user I dont know what it is but I'm in love y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2861</th>\n",
       "      <td>@user @user @user For having union representat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2862 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Tweets\n",
       "0     seeing ppl walking w/ crutches makes me really...\n",
       "1     look for the girl with the broken smile, ask h...\n",
       "2     Now I remember why I buy books online @user #s...\n",
       "3     @user @user So is he banded from wearing the c...\n",
       "4     Just found out there are Etch A Sketch apps.  ...\n",
       "...                                                 ...\n",
       "2857  I don't have to respect your beliefs.||I only ...\n",
       "2858  Women getting hit on by married managers at @u...\n",
       "2859  @user no but i followed you and i saw you post...\n",
       "2860  @user I dont know what it is but I'm in love y...\n",
       "2861  @user @user @user For having union representat...\n",
       "\n",
       "[2862 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mIrony Detection Test Subset:\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user Can U Help?||More conservatives needed o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Just walked in to #Starbucks and asked for a \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#NOT GONNA WIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@user He is exactly that sort of person. Weirdo!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>So much #sarcasm at work mate 10/10 #boring 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>If you drag yesterday into today, your tomorro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>Congrats to my fav @user &amp; her team &amp; my birth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>@user Jessica sheds tears at her fan signing e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>#Irony: al jazeera is pro Anti - #GamerGate be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>#NOT ALL üëå There good &amp; bad in every occupatio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>784 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Tweets\n",
       "0    @user Can U Help?||More conservatives needed o...\n",
       "1    Just walked in to #Starbucks and asked for a \"...\n",
       "2                                      #NOT GONNA WIN \n",
       "3    @user He is exactly that sort of person. Weirdo! \n",
       "4    So much #sarcasm at work mate 10/10 #boring 10...\n",
       "..                                                 ...\n",
       "779  If you drag yesterday into today, your tomorro...\n",
       "780  Congrats to my fav @user & her team & my birth...\n",
       "781  @user Jessica sheds tears at her fan signing e...\n",
       "782  #Irony: al jazeera is pro Anti - #GamerGate be...\n",
       "783  #NOT ALL üëå There good & bad in every occupatio...\n",
       "\n",
       "[784 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the Irony Detection Train subset (Feature variable (X))\n",
    "\n",
    "# Read a TXT file from internet (Github) that does not have a header\n",
    "df_irony_train_x = pd.read_csv('https://raw.githubusercontent.com/cardiffnlp/tweeteval/'\n",
    "                                 'main/datasets/irony/train_text.txt', header=None, sep ='\\t', names=['Tweets'])\n",
    "    \n",
    "print('\\n\\033[1mIrony Detection Train Subset:\\033[0m')\n",
    "display(df_irony_train_x) # Output: SemEval-2018 Irony Detection dataframe for training (truncated)\n",
    "\n",
    "\n",
    "# Load the Irony Detection Test subset (Feature variable (X))\n",
    "\n",
    "# Read a TXT file from internet (Github) that does not have a header\n",
    "df_irony_test_x = pd.read_csv('https://raw.githubusercontent.com/cardiffnlp/tweeteval/'\n",
    "                              'main/datasets/irony/test_text.txt', header=None, sep ='\\n', names=['Tweets'])\n",
    "  \n",
    "print('\\n\\033[1mIrony Detection Test Subset:\\033[0m')\n",
    "display(df_irony_test_x) # Output: SemEval-2018 Irony Detection dataframe for testing (truncated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task, the **Irony Detection train and test subset (labels)**  proposed in the TweetEval [[1]](#section_id) will be used. <br>\n",
    "Each dataset contains 2,862 tweets and 784 tweets correspondigly. They represent the target variable (Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mIrony Detection Train Subset (Labels):\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>irony_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2857</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2858</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2859</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2860</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2861</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2862 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      irony_output\n",
       "0                1\n",
       "1                0\n",
       "2                1\n",
       "3                1\n",
       "4                1\n",
       "...            ...\n",
       "2857             0\n",
       "2858             1\n",
       "2859             0\n",
       "2860             0\n",
       "2861             1\n",
       "\n",
       "[2862 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mIrony Detection Test Subset (Labels):\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>irony_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>784 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     irony_output\n",
       "0               0\n",
       "1               1\n",
       "2               0\n",
       "3               0\n",
       "4               1\n",
       "..            ...\n",
       "779             0\n",
       "780             0\n",
       "781             0\n",
       "782             1\n",
       "783             0\n",
       "\n",
       "[784 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the Irony Detection Train subset (Target Variable (Y))\n",
    "\n",
    "# Read a TXT file from internet (Github) that does not have a header\n",
    "df_irony_train_y = pd.read_csv('https://raw.githubusercontent.com/cardiffnlp/tweeteval/'\n",
    "                               'main/datasets/irony/train_labels.txt', header=None, names=['irony_output'])\n",
    "\n",
    "print('\\n\\033[1mIrony Detection Train Subset (Labels):\\033[0m')\n",
    "display(df_irony_train_y) # Output: SemEval-2018 Irony Detection dataframe for training (labels)(truncated)\n",
    "\n",
    "\n",
    "\n",
    "# Read a TXT file from internet (Github) that does not have a header\n",
    "df_irony_test_y = pd.read_csv('https://raw.githubusercontent.com/cardiffnlp/tweeteval/'\n",
    "                              'main/datasets/irony/test_labels.txt', header=None, names=['irony_output'])\n",
    "\n",
    "print('\\n\\033[1mIrony Detection Test Subset (Labels):\\033[0m')\n",
    "display(df_irony_test_y) # Output: SemEval-2018 Irony Detection dataframe for testing (labels)(truncated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mIrony Detection Train Subset:\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>irony_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>seeing ppl walking w/ crutches makes me really...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>look for the girl with the broken smile, ask h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Now I remember why I buy books online @user #s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@user @user So is he banded from wearing the c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just found out there are Etch A Sketch apps.  ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2857</th>\n",
       "      <td>I don't have to respect your beliefs.||I only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2858</th>\n",
       "      <td>Women getting hit on by married managers at @u...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2859</th>\n",
       "      <td>@user no but i followed you and i saw you post...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2860</th>\n",
       "      <td>@user I dont know what it is but I'm in love y...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2861</th>\n",
       "      <td>@user @user @user For having union representat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2862 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Tweets  irony_output\n",
       "0     seeing ppl walking w/ crutches makes me really...             1\n",
       "1     look for the girl with the broken smile, ask h...             0\n",
       "2     Now I remember why I buy books online @user #s...             1\n",
       "3     @user @user So is he banded from wearing the c...             1\n",
       "4     Just found out there are Etch A Sketch apps.  ...             1\n",
       "...                                                 ...           ...\n",
       "2857  I don't have to respect your beliefs.||I only ...             0\n",
       "2858  Women getting hit on by married managers at @u...             1\n",
       "2859  @user no but i followed you and i saw you post...             0\n",
       "2860  @user I dont know what it is but I'm in love y...             0\n",
       "2861  @user @user @user For having union representat...             1\n",
       "\n",
       "[2862 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mIrony Detection Test Subset:\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>irony_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user Can U Help?||More conservatives needed o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Just walked in to #Starbucks and asked for a \"...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#NOT GONNA WIN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@user He is exactly that sort of person. Weirdo!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>So much #sarcasm at work mate 10/10 #boring 10...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>If you drag yesterday into today, your tomorro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>Congrats to my fav @user &amp; her team &amp; my birth...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>@user Jessica sheds tears at her fan signing e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>#Irony: al jazeera is pro Anti - #GamerGate be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>#NOT ALL üëå There good &amp; bad in every occupatio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>784 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Tweets  irony_output\n",
       "0    @user Can U Help?||More conservatives needed o...             0\n",
       "1    Just walked in to #Starbucks and asked for a \"...             1\n",
       "2                                      #NOT GONNA WIN              0\n",
       "3    @user He is exactly that sort of person. Weirdo!              0\n",
       "4    So much #sarcasm at work mate 10/10 #boring 10...             1\n",
       "..                                                 ...           ...\n",
       "779  If you drag yesterday into today, your tomorro...             0\n",
       "780  Congrats to my fav @user & her team & my birth...             0\n",
       "781  @user Jessica sheds tears at her fan signing e...             0\n",
       "782  #Irony: al jazeera is pro Anti - #GamerGate be...             1\n",
       "783  #NOT ALL üëå There good & bad in every occupatio...             0\n",
       "\n",
       "[784 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Merging the X and Y dataframes (Training)\n",
    "df_irony_train = pd.concat([df_irony_train_x, df_irony_train_y], axis=1)  # Merging the dataframes\n",
    "print('\\n\\033[1mIrony Detection Train Subset:\\033[0m')\n",
    "display(df_irony_train) # Output: SemEval-2018 Irony Detection dataframe for training (truncated)\n",
    "\n",
    "\n",
    "# Merging the X and Y dataframes (Testing)\n",
    "df_irony_test = pd.concat([df_irony_test_x, df_irony_test_y], axis=1)  # Merging the dataframes\n",
    "print('\\n\\033[1mIrony Detection Test Subset:\\033[0m')\n",
    "display(df_irony_test) # Output: SemEval-2018 Irony Detection dataframe for testing (truncated)\n",
    "\n",
    "\n",
    "# .concact()      This function is used to concatenate two different dataframes.\n",
    "# Axis=1          This parameter indicates column-wise concatenation (Merging columns of two different dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      " seeing ppl walking w/ crutches makes me really excited for the next 3 weeks of my life   1 \n",
      "\n",
      "1 \n",
      " look for the girl with the broken smile, ask her if she wants to stay while, and she will be loved. üíïüéµ  0 \n",
      "\n",
      "2 \n",
      " Now I remember why I buy books online @user #servicewithasmile   1 \n",
      "\n",
      "3 \n",
      " @user @user So is he banded from wearing the clothes?  #Karma  1 \n",
      "\n",
      "4 \n",
      " Just found out there are Etch A Sketch apps.  #oldschool #notoldschool  1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exploring the first 5 rows of the Irony Detection Train subset\n",
    "\n",
    "# For-loop-enumerate iterates over indices (idx) and the first 5 rows (i) of a dataframe containing all the tweets (train subset)\n",
    "for idx, i in enumerate(range(5)):      \n",
    "    print(idx, '\\n', df_irony_train['Tweets'][i],\n",
    "         df_irony_train['irony_output'][i],'\\n')\n",
    "# Output: First 5 tweets in Irony Detection train subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Preprocessing Irony Detection Train and Test subset\n",
    "\n",
    "#### Cleaning Data (Part 1)\n",
    "The first preprocessing phase will consist in the following actions:\n",
    "\n",
    "|Action|Examples of the strings that will be removed or modified|\n",
    "|:--|:-------------------------------|\n",
    "|Lowercase the column \"Tweets\" | Can --> can, Texans --> texans, MLB --> mlb, Carly --> carly|\n",
    "|Remove Stopwords |'a', 'about', 'above', 'after', 'again' .... \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves''|\n",
    "\n",
    "1. In total, there are 179 stopwords in the NLTK module (stopwords.words('english')\n",
    "2. However, 326 stopwords were added to the list. \n",
    "3. In total, there are 505 stopwords in this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercasing the column \"Tweets\" (Irony Detection Train and Test subset) \n",
    "\n",
    "df_irony_train_c1 = df_irony_train.copy()                        # Create a copy of the Irony Detection Train subset\n",
    "df_irony_test_c1 = df_irony_test.copy()                          # Create a copy of the Irony Detection Test subset\n",
    "\n",
    "df_irony_train[\"Tweets\"] = df_irony_train[\"Tweets\"].str.lower()  # Lowercase the whole content of the column \"Tweets\" (Train)\n",
    "df_irony_test[\"Tweets\"] = df_irony_test[\"Tweets\"].str.lower()    # Lowercase the whole content of the column \"Tweets\" (Test)\n",
    "\n",
    "# Defining the Stopwords\n",
    "\n",
    "stopwords = [\"a's\", \"a\", \"about\", \"above\", \"according\", \"accordingly\", \"across\", \"actually\", \"after\", \"afterwards\", \"again\", \"against\",\n",
    " \"ain't\", \"ain\", \"all\", \"allow\", \"allows\", \"almost\", \"along\", \"already\", \"also\", \"although\", \"am\", \"among\", \"amongst\", \"an\", \"and\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \n",
    " \"apart\", \"appear\", \"appropriate\", \"are\", \"aren't\", \"aren\" \"around\", \"as\", \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \n",
    " \"available\", \"be\", \"because\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\", \n",
    " \"between\", \"beyond\", \"both\", \"brief\", \"but\", \"by\", \"c'mon\", \"c's\", \"came\", \"can\", \"can't\", \"cannot\", \"cant\", \"cause\", \n",
    " \"causes\", \"certain\", \"certainly\", \"clearly\", \"co\", \"com\", \"come\", \"comes\", \"concerning\", \"consequently\", \"consider\", \n",
    " \"considering\", \"contain\", \"containing\", \"contains\", \"corresponding\", \"could\", \"couldn't\", \"course\", \"currently\", \n",
    " \"definitely\", \"described\", \"despite\", \"did\", \"didn't\", \"different\", \"do\", \"does\", \"doesn't\", \"doing\", \"don't\", \"done\", \n",
    " \"down\", \"downwards\", \"during\", \"each\", \"edu\", \"eg\", \"either\", \"eight\", \"else\", \"elsewhere\", \"enough\", \"entirely\", \n",
    " \"especially\", \"et\", \"etc\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"exactly\", \n",
    " \"example\", \"far\", \"few\", \"fifth\", \"first\", \"five\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \n",
    " \"forth\", \"four\", \"from\", \"further\", \"furthermore\", \"get\", \"gets\", \"getting\", \"given\", \"go\", \"goes\", \"going\", \"gone\", \n",
    " \"got\", \"gotten\", \"had\", \"hadn't\", \"happens\", \"hardly\", \"has\", \"hasn't\", \"have\", \"haven't\", \"having\", \"he\", \"he's\", \n",
    " \"hence\", \"her\", \"here\", \"here's\", \"hereafter\",\"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \n",
    " \"hither\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"ie\", \"if\", \"immediate\", \"in\", \n",
    " \"inasmuch\", \"inc\", \"indeed\", \"indicate\", \"indicated\", \"indicates\", \"inner\", \"insofar\", \"instead\", \"into\", \"inward\", \"is\",\n",
    " \"isn't\", \"it\", \"it'd\", \"it'll\", \"it's\", \"its\", \"itself\", \"just\", \"keep\", \"keeps\", \"kept\", \"know\", \"known\", \"knows\", \n",
    " \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"least\", \"less\", \"lest\", \"let\", \"let's\", \"likely\", \"little\", \"look\", \n",
    " \"looking\", \"looks\", \"ltd\", \"mainly\", \"many\", \"may\", \"maybe\", \"me\", \"meanwhile\", \"merely\", \"might\", \"more\" , \"moreover\", \n",
    " \"most\", \"mostly\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"nd\", \"near\", \"nearly\", \"necessary\", \"need\", \"needs\",\n",
    " \"neither\", \"never\", \"nevertheless\",\"next\", \"nine\", \"no\", \"nobody\", \"non\", \"none\", \"noone\", \"nor\", \"normally\", \"not\",\n",
    " \"nothing\", \"novel\", \"now\", \"nowhere\", \"obviously\", \"of\", \"off\", \"often\", \"oh\", \"ok\", \"okay\", \"on\", \"once\", \"one\", \n",
    " \"ones\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \n",
    " \"over\", \"overall\", \"own\", \"particular\", \"particularly\", \"per\", \"perhaps\", \"placed\", \"plus\", \"possible\", \"presumably\", \n",
    " \"probably\", \"provides\", \"que\", \"quite\", \"qv\", \"rather\", \"rd\", \"re\", \"really\", \"reasonably\", \"regarding\", \"regardless\", \n",
    " \"regards\", \"relatively\", \"respectively\", \"right\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"second\", \"secondly\", \n",
    " \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sent\", \"seriously\", \"seven\", \"several\", \n",
    " \"shall\", \"she\", \"should\", \"shouldn't\", \"since\", \"six\", \"so\", \"some\", \"somebody\", \"somehow\", \"someone\", \"something\", \n",
    " \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"specified\", \"specify\", \"specifying\", \"still\", \"sub\", \"such\", \n",
    " \"sup\", \"t's\", \"take\", \"taken\", \"tell\", \"tends\", \"th\", \"than\",\"that\", \"that's\" , \"thats\", \"the\", \"their\", \"theirs\", \"them\", \n",
    " \"themselves\", \"then\", \"thence\", \"there\", \"there's\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"theres\", \"thereupon\",\n",
    " \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \n",
    " \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"took\", \"toward\", \"towards\", \"tried\", \n",
    " \"tries\", \"truly\", \"try\", \"trying\", \"twice\", \"two\", \"un\", \"under\" , \"unless\", \"unlikely\", \"until\", \"unto\", \"up\", \"upon\", \n",
    " \"use\", \"used\", \"useful\", \"uses\", \"using\", \"usually\", \"value\", \"various\", \"very\", \"via\", \"viz\", \"vs\", \"want\", \"wants\", \n",
    " \"was\", \"wasn't\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"weren't\" , \"what\", \"what's\", \"whatever\", \"when\", \n",
    " \"whence\", \"whenever\", \"where\", \"where's\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \n",
    " \"whether\", \"which\", \"while\", \"whither\", \"who\", \"who's\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"willing\", \n",
    " \"with\", \"within\", \"without\", \"won't\", \"wonder\", \"would\", \"wouldn't\", \"yes\", \"yet\", \"you\", \"you'd\", \"you'll\", \"you're\", \n",
    " \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"zero\", \"'s'\"]\n",
    "\n",
    "\n",
    "# .copy()          This function is used to make a copy of one dataframe with indices and data\n",
    "# df.str.lower()   This function is used to transform the content of one column or dataframe to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Stopwords from the Irony Detection Train subset\n",
    "\n",
    "for i in stopwords:         # For-loop iterates over all the words found in the list \"stopwords\"\n",
    "    df_irony_train['Tweets'] = df_irony_train['Tweets'].replace(to_replace=r\"\\b%s\\b\"%(i), value='', regex=True)\n",
    "# Action: Replace the stopwords by an empty character ('') (train)\n",
    "\n",
    "\n",
    "# Removing Stopwords from the Irony Detection Test subset\n",
    "\n",
    "for i in stopwords:         # For-loop iterates over all the words found in the list \"stopwords\"\n",
    "    df_irony_test['Tweets'] = df_irony_test['Tweets'].replace(to_replace=r\"\\b%s\\b\"%(i), value='', regex=True)\n",
    "# Action: Replace the stopwords by an empty character ('') (test)\n",
    "\n",
    "\n",
    "# df.replace()   This function is used to replace occurrences of a particular sub-string with another sub-string.\n",
    "#                In this case, the ReGex %s has been replaced by an empty character ('')\n",
    "# To_replace     This parameter indicates the sub-string to replace\n",
    "# value          This parameter indicates the sub-string to replace with\n",
    "# ReGex=True     This parameter indicates that the sub-string to replace is a Regular Expression\n",
    "\n",
    "# ReGex Explanation\n",
    "# \\bstring\\b    This RegEx matches only the string declared.\n",
    "#               In this case, it matches the variable string character (%s), which contains any of the stopwords declared.\n",
    "#               For example. it will match either 'about', where' or 'has'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the original Tweet vs the Tweet after first preprocessing\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None, 'display.max_colwidth', None)\n",
    "# This option is used to print the entire Pandas dataframe (all rows, all columns & all content)\n",
    "\n",
    "# Comparing the Original Tweet VS the Tweet after first preprocessing (train subset)\n",
    "print('\\n\\033[1mIrony Detection Train Subset:\\033[0m')\n",
    "display(df_irony_train_c1[['Tweets']].iloc[0:3]) # Output: Original Tweet \n",
    "display(df_irony_train[['Tweets']].iloc[0:3]) # Output: Tweet after preprocessing \n",
    "\n",
    "\n",
    "# Comparing the Original Tweet VS the Tweet after first preprocessing (Test subset)\n",
    "print('\\n\\033[1mIrony Detection Test Subset:\\033[0m')\n",
    "display(df_irony_test_c1[['Tweets']].iloc[0:3]) # Output: Original Tweet\n",
    "display(df_irony_test[['Tweets']].iloc[0:3]) # Output: Tweet after preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, the first 2 preprocessing techniques has been applied succesfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Data (Part 2)\n",
    "The second preprocessing phase will consist in the following actions:\n",
    "\n",
    "|Action|Examples of the strings that will be removed or modified|\n",
    "|:--|:-------------------------------|\n",
    "|Remove User Objects |@user, @user_1, @user-1, @paulina_100, @WiNer206|\n",
    "|Remove Hashtags | #friends #bff #celebrate #sandiego #sundayfunday #ObsessedWithMyDog|\n",
    "|Remove Non-ASCII Characters |ÎûôÎ∞î, ÏóêÏù¥Ïò§, ·¥¨·¥∫·¥º·µÄ·¥¥·¥±·¥ø\t·¥∞·¥ø·¥µ·¥∫·¥∑\t·¥¥·¥¨·¥æ·¥æ·µû, ‡∏ö‡∏°‡∏≤‡πÅ‡∏•, –¥–æ–±—Ä–æ–π–Ω–æ—á–∏|\n",
    "|Remove new line characters|\\n|\n",
    "|Remove punctuation marks |!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~|\n",
    "|Remove Two or more spaces|&ensp;, &ensp;&ensp;, &ensp;&ensp;&ensp;, &ensp;&ensp;&ensp;&ensp;,|\n",
    "|Remove 1 or more underscores|\"____ \"_____\" \"_____________\"|\n",
    "|Remove numerical characters|0007, 0, 12389, 50000|\n",
    "|Stemming|walking --> walk, excited -->, excit, kids --> kid|\n",
    "\n",
    "1. The **Punkt/english.pickle** Sentence Tokenizer divides a text into a list of sentences by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start a sentence. <br>\n",
    "2. The Punkt Sentence Tokenizer is based on the publication by [Kiss, T. & Strunk, J., 2006. Unsupervised Multilingual Sentence Boundary Detection. Computational Linguistics, 32(4), pp. 485-525](https://direct.mit.edu/coli/article/32/4/485/1923/Unsupervised-Multilingual-Sentence-Boundary)\n",
    "3. SnowballStemmer is a stemming algorithm used to remove morphological affixes from words, leaving only the word stem. \n",
    "4. SnowballStemmer is part of the NLTK libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to clean data\n",
    "\n",
    "def split_sentences(text):\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle') # Get the The Punkt Sentence Tokenizer. \n",
    "    list_df = []                             # Create a new-empty list to store all sentences after preprocessing\n",
    "    mark_sentence = '***'.join(tokenizer.tokenize(text))  \n",
    "    # Add a sentence delimitator (***) to identify the beginning of each sentence\n",
    "    \n",
    "    #mark_sentence = re.sub(r\"([a-z])\\1+\", r'\\1', mark_sentence)\n",
    "    # ReGex that removes duplicate letters. Replace them by the word without duplications)\n",
    "    \n",
    "    mark_sentence = re.sub(r\"([@#][\\w_-]+)\", '', mark_sentence) \n",
    "    # ReGex that remove User Objects. (Replace them by Nothing (''))\n",
    "    \n",
    "    mark_sentence = re.sub(r\"([^\\x00-\\x7F]+)\", '', mark_sentence) \n",
    "    # ReGex that removes Non-ASCII Characters. (Replace them by nothing (''))\n",
    "    \n",
    "    mark_sentence = re.sub(r\"([\\b_\\b]{1,})\", '', mark_sentence) \n",
    "    # ReGex that removes 1 or more underscores. (Replace them by nothing (''))\n",
    "    \n",
    "    mark_sentence = re.sub(r\"([0-9])\", '', mark_sentence) \n",
    "    # ReGex that removes numbers. (Replace them by nothing (''))\n",
    "    \n",
    "    mark_sentence = re.sub(r\"(\\n+)\", '', mark_sentence)\n",
    "    # ReGex that removes new line characters (Replace them by nothing ('')\n",
    "    \n",
    "    mark_sentence = re.sub(r\"([^\\w\\s])\", ' ', mark_sentence) \n",
    "    # ReGex that removes punctuaction marks (Replace them by space (' '))\n",
    "    \n",
    "    mark_sentence = re.sub(r\"([\\s]{2,})\", ' ', mark_sentence)\n",
    "    # ReGex that removes Two or more spaces (Replace them by space (' '))\n",
    "    \n",
    "    list_df  = mark_sentence.split('***')   # Perform the sentence segmentation\n",
    "    return list_df\n",
    "\n",
    "print('Function split_sentences has been succesfully created')\n",
    "\n",
    "\n",
    "# ReGex Explanation:\n",
    "# \\s    This RegEx matches any whitespace character. In other words, it will find strings such as \" \", \"\\r\" or \"\\n\"\n",
    "# \\w    This ReGex matches any alphamumeric character. In other words, it will find strings such as: \"a\", \"julio\", \"100\", julio100\"\n",
    "# ()    The parenthesis identify a group of characters formed by the combination of 1, 2 or more characters.\n",
    "#       For example, ([@][\\w_-]) represents 1 group that has an structure \"@ + Alphabetic Characters + _ or -\"\n",
    "# []    The brackets [] identify a range of characters.\n",
    "#       For example, [\\w\\s] represents 1 range of values from any alphanumeric character or any whitespace character.\n",
    "# ^     The caret symbol (^) inside of a character set [] represents the characters NOT in the range [\\w\\s]. \n",
    "#       (Different to the range [\\w\\s])\n",
    "# \\n    This character represents the new line character\n",
    "# +     The plus sign (+) declares that \"\\n\" is compulsory and should appears at least once. (1)\n",
    "# {2,}  This quantifier declares that \"\\s\" is compulsory and should appears at least two times. (2)\n",
    "# [0-9] This ReGex matches any numerical character (0-9). In other words, it will find numbers such as \"007\" or \"0\"\n",
    "# \\bstring\\b    This RegEx matches only the string declared.\n",
    "#               In this case, it matches the underscore (_)\n",
    "\n",
    "# \\\\X00      This means 0 in Hexa-decimal connotation\n",
    "# \\\\7F       This means 127 in Hexa-decimal connotation.\n",
    "# \\x00-\\x7F  This means a range from 0 to 127, which represents the range of the ASCII characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the data (Part 2)(Train subset)\n",
    "\n",
    "# Run the function \"split_sentences\" on the Train subset \n",
    "df_irony_train['clean_tweet'] = df_irony_train['Tweets'].apply(split_sentences)\n",
    "# Action: Create a column 'clean_tweet' that stores the Tweets after preprocessing (training)\n",
    "\n",
    "df_irony_train['clean_tweet'] = df_irony_train['clean_tweet'].apply(lambda x: ','.join(map(str, x)))\n",
    "# Action: Transform the list obtained after preprocessing into single strings. (training)\n",
    "\n",
    "df_irony_train['clean_tweet'] = df_irony_train['clean_tweet'].str.strip()\n",
    "# Action: Remove extra spaces at the beginning and the end of any cell. (training)\n",
    "\n",
    "\n",
    "# Cleaning the data (Part 2)(test subset)\n",
    "\n",
    "# Run the function \"split_sentences\" on the Test subset\n",
    "df_irony_test['clean_tweet'] = df_irony_test['Tweets'].apply(split_sentences)\n",
    "# Action: Create a column 'clean_tweet' that stores the Tweets after preprocessing (test)\n",
    "\n",
    "df_irony_test['clean_tweet'] = df_irony_test['clean_tweet'].apply(lambda x: ','.join(map(str, x)))\n",
    "# Action: Transform the list obtained after preprocessing into single strings. (test)\n",
    "\n",
    "df_irony_test['clean_tweet'] = df_irony_test['clean_tweet'].str.strip()\n",
    "# Action: Remove extra spaces at the beginning and the end of any cell. (test)\n",
    "\n",
    "print ('Done')\n",
    "\n",
    "# df.apply()    This command is used to pass a function and apply it on every single value of the column or dataframe.\n",
    "# lamda()       This command is only useful when we want to define a function that will be used only once in our program.\n",
    "# ','.join()    This function takes all items in an iterable and joins them into one string.\n",
    "#               In this case, it will take any word and seperate it with \",\". For example: ['irwin','arnstein','subject']\n",
    "# map()         This function is used to replace each value in a column or dataframe with another value.\n",
    "#               In this case, this function transformed the list created after preprocessing into single strings\n",
    "# str.strip()   This function is used to remove spaces at the beginning and the end of the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming the data\n",
    "\n",
    "stemmer = SnowballStemmer('english') # Create a Stemmer object for 'English' language\n",
    "\n",
    "# Stemming the data (Train subset)\n",
    "df_irony_train['clean_tweet1'] = df_irony_train['clean_tweet'].apply(lambda x: [stemmer.stem(word) for word in x.split()])\n",
    "# Action: Stem every word found in every row (train)\n",
    "\n",
    "df_irony_train['clean_tweet1'] = df_irony_train['clean_tweet1'].apply(lambda x: ' '.join(map(str, x)))\n",
    "# Action: Transform the list obtained after preprocessing into single strings (train)\n",
    "\n",
    "df_irony_train = df_irony_train.drop(columns=['clean_tweet'])\n",
    "# Drop the column \"clean_tweet\" (unstemmed column)(train)\n",
    "\n",
    "\n",
    "\n",
    "# Stemming the data (Test subset)\n",
    "df_irony_test['clean_tweet1'] = df_irony_test['clean_tweet'].apply(lambda x: [stemmer.stem(word) for word in x.split()]) \n",
    "# Action: Stem every word found in every row (test)\n",
    "\n",
    "df_irony_test['clean_tweet1'] = df_irony_test['clean_tweet1'].apply(lambda x: ' '.join(map(str, x)))\n",
    "# Action: Transform the list obtained after preprocessing into single strings (test)\n",
    "\n",
    "df_irony_test = df_irony_test.drop(columns=['clean_tweet']) # Get rid of the unstemmed column.\n",
    "# Drop the column \"clean_tweet\" (unstemmed column)(test)\n",
    "\n",
    "print ('Done')\n",
    "\n",
    "# df.apply()    This command is used to pass a function and apply it on every single value of the column or dataframe.\n",
    "# lamda()       This command is only useful when we want to define a function that will be used only once in our program.\n",
    "# x.split()     This function is used to split a string into a list using a user specified separator.\n",
    "#               In this case, the content of each row is divided into words using a whitespace separator (' ')\n",
    "# ','.join()    This function takes all items in an iterable and joins them into one string.\n",
    "#               In this case, it will take any word and seperate it with \",\". For example: ['irwin','arnstein','subject']\n",
    "# map()         This function is used to replace each value in a column or dataframe with another value.\n",
    "#               In this case, this function transformed the list created after preprocessing into single strings\n",
    "# drop(columns='')  This function is used to delete 1 specific column\n",
    "# stem()       This function is used to execute the stemmer object, which in this case is the NLTK Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the original Tweet vs the Tweet after second preprocessing\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None, 'display.max_colwidth', None)\n",
    "# This option is used to print the entire Pandas dataframe (all rows, all columns & all content)\n",
    "\n",
    "# Train subset\n",
    "print('\\n\\033[1mEmotion Prediction Train Subset:\\033[0m')\n",
    "display(df_emotion_train[0:5]) # Output: First 5 tweets (train)\n",
    "\n",
    "# Test subset\n",
    "print('\\n\\033[1mEmotion Prediction Test Subset:\\033[0m')\n",
    "display(df_emotion_test[0:5]) # Output: First 5 tweets (test)\n",
    "\n",
    "# Column 'Tweets' contains the Original tweets\n",
    "# Column 'clean_tweet1' contains the Tweets after all preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the final Emotion Prediction subset after preprocessing\n",
    "\n",
    "pd.reset_option('^display.', silent=True) # Reset the default Pandas Display (Truncated)\n",
    "\n",
    "# Final emotion prediction Train subset\n",
    "print('\\n\\033[1mEmotion Prediction Train Subset:\\033[0m')\n",
    "df_emotion_train_final = df_emotion_train[['clean_tweet1','Sent_label']] \n",
    "# Create the final Emotion Prediction dataframe (train)\n",
    "\n",
    "display(df_emotion_train_final) # Output: Emotion prediction train subset (cleaning)\n",
    "\n",
    "# Final emotion prediction Test subset\n",
    "print('\\n\\033[1mEmotion Prediction test Subset:\\033[0m')\n",
    "df_emotion_test_final = df_emotion_test[['clean_tweet1','Sent_label']]\n",
    "# Create the final Emotion Prediction dataframe (test)\n",
    "\n",
    "display(df_emotion_test_final)  # Output: Emoji prediction test subset (cleaning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the Emotion Prediction subsets to Python lists.\n",
    "\n",
    "# Train subset\n",
    "list_emotion_train_final = df_emotion_train_final['clean_tweet1'].tolist()\n",
    "# Action: Transform the Emotion Prediction Train dataframe into a Python list\n",
    "\n",
    "# Test subset\n",
    "list_emotion_test_final = df_emotion_test_final['clean_tweet1'].tolist()\n",
    "# Action: Transform the Emotion Prediction Test dataframe into a Python list\n",
    "\n",
    "display(list_emotion_train_final) # Output: Emotion Prediction Train list\n",
    "#Uncomment the following line if you want to see the Emotion Prediction Test list\n",
    "#display(list_emotion_test_final) \n",
    "\n",
    "# df.values.tolist()    This function is used to convert a dataFrame into a Python list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Performing Word Embeddings\n",
    "\n",
    "Word embeddings is nothing but the process of converting text data to numerical vectors, and it is used to capture not only  the semantic of the word, but also their emotional content. <br>\n",
    "\n",
    "In this case, the **Bag-of-Words** technique will be applied in this project.\n",
    "1. BoW is a representation of text that describes the occurrence of words within a document collection.\n",
    "2. BoW uses word occurrence frequencies to measure the content of the tweet and see how often each words appeared\n",
    "3. BoW is a method to extract features (X) from tweets. These attributes should be used for training any Machine Learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing the Emoji Prediction Train subset\n",
    "\n",
    "#Uncomment the following line if you want to print all columns and all content of a given dataframe. \n",
    "# (Truncated rows, all columns & all content)\n",
    "#pd.set_option(\"display.max_columns\", None, 'display.max_colwidth', None)\n",
    "\n",
    "vect_train = CountVectorizer(dtype=np.uint8, max_features = 373, min_df = 5)\n",
    "Z_train = vect_train.fit_transform(list_emotion_train_final)\n",
    "df_vect_train = pd.DataFrame(Z_train.A, columns=vect_train.get_feature_names())\n",
    "display(df_vect_train)\n",
    "\n",
    "print(\"--------------------------------------------------\")\n",
    "display(list_emotion_train_final[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing the Test subset\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None, 'display.max_colwidth', None)\n",
    "vect_test= CountVectorizer(dtype=np.uint8, max_features = 900, min_df = 5)\n",
    "Z_test = vect_test.fit_transform(list_emotion_test_final)\n",
    "df_vect_test = pd.DataFrame(Z_test.A, columns=vect_test.get_feature_names())\n",
    "display(df_vect_test)\n",
    "\n",
    "print(\"--------------------------------------------------\")\n",
    "display(list_emotion_test_final[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfering the data into 2 numpy arrays\n",
    "\n",
    "x_train = df_vect_train.loc[:,:].to_numpy() \n",
    "# This array contains all the feature variable values.\n",
    "\n",
    "y_train = df_emotion_train_final.loc[:, df_emotion_train_final.columns == 'Sent_label'].to_numpy()\n",
    "# This array contains ONLY the target variable values.\n",
    "\n",
    "print('\\n'+'\\033[1m'+'Array of feature variables (Train):'+'\\033[0m', x_train.shape) \n",
    "print(x_train) # Output: All possible elements of the array. (This includes everything except the values of the target variable (\"Class label\")\n",
    "\n",
    "print('\\n'+'\\033[1m'+'Array of target variable (Train):'+'\\033[0m', y_train.shape)\n",
    "print(y_train) # Output: All possible elements of the array. (This includes only values of \"Class Label\" feature from the wine dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfering the data into 2 numpy arrays\n",
    "\n",
    "x_test = df_vect_test.loc[:,:].to_numpy() \n",
    "# This array contains all the feature variable values.\n",
    "\n",
    "y_test = df_emotion_test_final.loc[:, df_emotion_test_final.columns == 'Sent_label'].to_numpy()\n",
    "# This array contains ONLY the target variable values.\n",
    "\n",
    "print('\\n'+'\\033[1m'+'Array of feature variables (Test):'+'\\033[0m', x_test.shape) \n",
    "print(x_test) # Output: All possible elements of the array. (This includes everything except the values of the target variable (\"Class label\")\n",
    "\n",
    "print('\\n'+'\\033[1m'+'Array of target variable (Test):'+'\\033[0m', y_test.shape)\n",
    "print(y_test) # Output: All possible elements of the array. (This includes only values of \"Class Label\" feature from the wine dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Building the Machine Learning model\n",
    "\n",
    "#### Naƒ±ve  Bayes  Multinomial  classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a Naive Bayes model \n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "\n",
    "mnb = MultinomialNB() \n",
    "\n",
    "# Train the model\n",
    "mnb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the model that was trained on the X_train_cv data and apply it to the X_test_cv data \n",
    "y_pred_cv_mnb = mnb.predict(x_test) \n",
    "y_pred_cv_mnb # The output is all of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "print(accuracy_score(y_test, y_pred_cv_mnb))\n",
    "print(classification_report(y_test, y_pred_cv_mnb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Logistic Regression object\n",
    "logr = LogisticRegression(solver='lbfgs')  \n",
    "# Train the model\n",
    "logr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the model that was trained on the X_train_cv data and apply it to the X_test_cv data \n",
    "y_pred_cv_logr = logr.predict(x_test) \n",
    "y_pred_cv_logr # The output is all of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test, y_pred_cv_logr))\n",
    "print(classification_report(y_test, y_pred_cv_logr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "<a id='section_id'></a>\n",
    "&emsp;[1] C. Van Hee, E. Lefever, and V. Hoste, ‚ÄúSemeval-2018 task 3: Irony detection in english tweets,‚Äù in Proceedings of The 12th  International Workshop<br>\n",
    "&emsp;&emsp;&ensp;on Semantic Evaluation, 2018, pp. 39‚Äì50<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
